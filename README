This project is a student project related to the study of Machine Learning. Specifically, this project tries to build the fastest implementation of Parametrized Rocchio Classifier (PRC) that is described in this paper: http://disi.unitn.it/moschitti/Projects/ECIR03.pdf (Alessandro Moschitti. 2003. A study on optimal parameter tuning for Rocchio text classifier. In Proceedings of the 25th European conference on IR research (ECIR'03), Fabrizio Sebastiani (Ed.). Springer-Verlag, Berlin, Heidelberg, 420-435.) More information on Rocchio can be obtained from Prof. Moschitti's teaching website: http://disi.unitn.it/moschitti/teaching.html, specifically the slide on Automated Text Categorization.

See INSTALL to know how to build the codebase and use it.
See doc/ for the documentations and further information.
See HACKING for some suggested ways to debug and develop the codebase further.
See COPYING for the LICENSE of the codebase.
See below for the experiment result.

Below are some experiment results that I obtain using the following setup:
1. A*Note Laptop (http://0x657573.wordpress.com/2010/12/12/finding-another-free-software-compatible-x86-laptop-anote/)
* Intel Pentium Dual Core T3400 (1MB L2 cache, 2.16 GHz, 667 MHz FSB)
* 2GB DDR2 RAM
* Intel 965GM chipset, X3100GMA

2. Software
* GNU/Linux Ubuntu 9.04 operating system
* GNU C Library 2.9
* GCC 4.3.3

3. Compilation
* Using -O3 and -DBUFFER_SIZE=4096

I measure the computation performance in terms of the processing time, the CPU usage and the ratio of the process's CPU seconds spent in user mode and in system mode (i.e., user/sys). While CPU usage can indicate how efficient a program does its calculation, a low (i.e., less than or equal to 1) user/sys ratio can indicate performance bottle-neck like doing output within a loop, frequent swapping to disk, frequent page fault, frequent user space to kernel space switching, which is rather expensive, and the like. On the other hand, having user/sys ratio nearing infinity does not always indicate a good performance because an intensive computation in the user space may hide the existence of the aforementioned performance bottle-necks (see doc/output_kills/README).

I observe that performance really suffers when doing output but not when doing input. Beside this, I also observe that C file outputting facility (e.g., fprintf) has much better performance compared to that of C++ (e.g., doing `myfile << 0.2322;'). The details of the experiment that leads to this conclusion can be found in doc/output_kills/README.

Due to not checking the behavior of `xargs', I only discovered much later that processing unit w_to_vector did not receive all documents to be processed because there is a limit on the number of command line arguments that can be passed to a process. Specifically, only 3,613 weight vectors were stored in w_vectors.bin to be processed by processing unit rocchio because only 3,613 files are given by `xargs' as command line arguments. This problem can be prevented by forcing `xargs' to quit using `-x' if the number of command line arguments exceeds the limit.

Aside from this, even if `xargs' did not present that limitation, w_to_vector actually could not output all weight vectors of all documents. This is because of file size limit of the operating systems. Since each weight vector has 32,002 elements and the total number of documents in the training set is 11,476, the minimum size of file w_vectors.bin is 8 bytes x 32,002 x 11,476 = 2,938,039,616 bytes, which is approximately 3 GB. However, because the weight vectors are sparse vectors, an efficient data structure and computation can be devised. As advised by Prof. Moschitti, the elements of a weight vector can be put in a binary search tree.

Experiment results:
1. On Reuters21578-Apte-115Cat (http://disi.unitn.it/moschitti/corpora/Reuters21578-Apte-115Cat.tar.gz)

* The performance of reference of implementation (ROI) provided by Prof. Moschitti:
** Step 1. Tokenization using my tokenizer because ROI does not provide the tokenizer:
time ( for directory in `ls $result_dir`; do
    cat=$training_dir/$directory
    for file in `ls $cat`; do
 	( $tokenizer $cat/$file \
	    | sed -e 's%.*%'$file'\t&\t1\t1%' > $cat/$file.tok) &
    done
done; wait )

time ( cd $result_dir \
     && ( find $training_dir -mindepth 1 -maxdepth 1 -type d \
     	  | sed -e 's%.*/\(.*\)%\1%' \
     	  | xargs mkdir ) \
     && ( for directory in `ls $result_dir`; do
     	      cat=$training_dir/$directory
	      for file in `ls $cat`; do
	      	  ( $tokenizer $cat/$file \
		    | sed -e 's%.*%'$file'\t&\t1\t1%' \
		      > $result_dir/$directory/$file.tok ) &
	      done
	      wait
	      cat $result_dir/$directory/*.tok \
	      	  > $result_dir../clusteringCategories/$directory.le
	  done ) \
     && cd .. )
results in 33.004s at CPU usage of 100.00% with user/sys ratio of 0.638.

** Step 2. Lemma unification
bin/TCF UNI -RCclusteringCategories
takes 9.540s at CPU usage of 86.75% with user/sys ratio of 4.146.

** Step 3. Building class centroids
bin/TCF CCE -SP20 -SE1
takes 13.390s at CPU usage of 96.47% with user/sys ratio of 7.409.

** Step 4. Building global centroids
bin/TCF GCE -DF0
takes 0.306s at CPU usage of 94.26% with user/sys ratio of 23.000.

** Step 5. Dictionary construction
bin/TCF DIC -GA0
takes 6.689s at CPU usage of 97.18% with user/sys ratio of 4.062.

** Step 6. Classifying the 
bin/TCF CLA -BP > BEP
takes 1.809s at CPU usage of 94.64% with user/sys ratio of 5.028 and obtains:
MacroAverage: Recall: 0.617557, Precision: 0.833793
MicroAverage: Recall: 0.710041, Precision: 0.723382, f1: 0.716649

** Total running time is 64.738s.

* My implementation
** Step 1. Tokenization and TF calculation takes 23.780s at CPU usage of 100.00% with user/sys ratio of 0.435. Below are some further experiments of different approaches:
*** Without parallelism,
"$exec_dir"/tokenizer $cat/$file | "$exec_dir"/tf > "$result_dir"/$directory/$file.tf
results in:
real	1m33.018s
user	0m31.930s
sys	0m53.555s

*** With parallelism using `sed' as the tokenizer,
( sed -e 's%[] ,.;:?!<>()["'\''{}\r\n\t\v-]%\n%g' $cat/$file | $tf -o $result_dir/$directory/$file.tf) &
results in 40.681s at CPU usage of 100.00% with user/sys ratio of 0.620.

*** With parallelism,
( $exec_dir/tokenizer $cat/$file | $exec_dir/tf -o $result_dir/$directory/$file.tf) &
results in:
real	0m30.183s
user	0m16.641s
sys	0m33.690s

*** Using bash -c is slower as shown below.
bash -c $exec_dir'/tokenizer '$cat/$file' | '$exec_dir'/tf -o '$result_dir/$directory/$file.tf &
results in:
real	0m53.261s
user	0m37.258s
sys	0m52.531s

*** Combining tokenizer and tf into a single process results in 25% speed up.
( "$exec_dir"/tok_tf $cat/$file > "$result_dir"/$directory/$file.tf ) &
real	0m22.627s
user	0m11.625s
sys	0m22.585s

*** Without parallelism, tokenizer and tf as one process is still poor.
"$exec_dir"/tok_tf $cat/$file > "$result_dir"/$directory/$file.tf
real	1m10.243s
user	0m23.973s
sys	0m43.395s

*** To conclude, parallelism is the way to go and piping surely incurs overhead that is justified by modifiability reason (e.g., adding stop list processing unit is very easy when pipe is used).

** Step 2. IDF calculation and dictionary building takes 3.190s at CPU usage of 96.07% with user/sys ratio of 9.788.

** Step 3. Generation of w vectors for all documents takes 1.471s at CPU usage of 97.63% with user/sys ratio of 7.975 after harnessing sparse vector representation and taking file names to be processed from standard input or a file instead of passing the file names as a command-line argument. Previously, without sparse vector representation and by passing the file names as a command-line argument, it took 149.705s at CPU usage of 35.80% with user/sys ratio of 1.738 and only 3,613 documents are processed due to the limitation of the number command line arguments that can be passed into a process as described above. Below are some experiments of different approaches:
*** Using socket in which idf_dic processing unit acts as a server and several w_to_vector processing units act as clients in which each w_to_vector processing unit takes care of only one document (see https://github.com/eus/tc_rocchio/commit/24b7db191e82d6d3f694dd6828b9cd7af0061e80),
( $exec_dir/w_to_vector -D /tmp/tc_rocchio/idf_dic.socket -o $file.w $file ) &
results in:
real	3m21.748s
user	1m34.854s
sys	3m19.760s

*** If socket is removed and each w_to_vector processing unit loads IDF_DIC file directly, the IDF_DIC file load time is just too much as shown below.
real	9m45.863s
user	14m54.820s
sys	2m5.164s

*** Using only one w_to_vector processing unit to load IDF_DIC file once and process all TF documents serially takes 138.011s at CPU usage of 38.31% which is 32% speed up. But, I notice that this processing unit does not have a stable performance. Last time it was around 178s. After that, it was around 150s. Although previously I thought that this was because the processing unit processes the input files one-by-one resulting in the OS scheduler disadvantaging this processing unit, it is clear now that this is because the number of command line arguments are limited by xargs resulting in more documents in one case and less documents in the other case due to different length of the absolute path of file names as a result of putting the intermediate files in deeper subdirectories in the latter case.

** Step 4. Generation of PRCs (Parametrized Rocchio Classifiers), in which the classifier of any category is a binary classifer consisting of a W vector and a threshold, for all categories using 20 random (the random seed is 1) estimation sets containing 30% of training data and trying 31 different values of P from 0 to 30 with increment 1 takes 15m8.926s at CPU usage of 99.07% with user/sys ratio of 270.245. Specifically, without using any ES (i.e., only working on LS) and by using P = 1, the generation of all classifiers on the whole training set (LS) takes 5.168s at CPU usage of 98.29% with user/sys ratio of 44.357. Adding one ES but using only one value of P = 1 takes 6.974s at CPU usage of 97.11% with user/sys ratio of 75.954. In other words, given a single value of P, one ES processing takes about 1.806s seconds (6.974s - 5.168s). The final result confirms that the average time taken by one ES processing involving only one value of P is around that ((908.926s [total time spent on all ESes for all P values and LS] - 5.168s [time spent on LS]) / 20 [the number of ESes] / 31 [the number of P values] = 1.46s).

Calculating only the W vectors of all categories using a fixed value of P and without doing any threshold estimation takes 1.384s at CPU usage of 97.68% with user/sys ratio of 47.285 by using sparse vector representation. Previously without sparse vector representation, it was 4.732s at CPU usage of 97.22% but with a fatal logic error in which calculation of a profile vector is prematurely aborted by using `goto output' once an element in the vector is negative. Aside from that, only 3,613 weight vectors were processed due to an error in the processing unit w_to_vector.

** Step 5. Classifying the training set itself takes 5.877s at CPU usage of 99.65% with user/sys ratio of 182.000 and obtains:
[MacroAverage] Recall: 0.853578, Precision: 0.853606
[MicroAverage] Recall: 0.833351, Precision: 0.833351, f1: 0.833351

Previously in the presence of logic error due to my misunderstanding of OVA, it was 6.816s at CPU usage of 96.00% with user/sys ratio of 76.904 by using sparse vector representation and taking file names to be processed from standard input or a file. Previously without sparse vector representation and by passing the file names as a command-line argument, it took 32.982s at CPU usage of 93.60% with user/sys ratio of 42.362 although only 3,613 weight vectors were considered. Previoulsy I thought that the slowness was due to either output operations or not harnessing sparse vectors. Now I can conclude that it is because of not harnessing sparse vectors.

** Step 6. Tokenization and TF calculation of the test set takes 7.564s at CPU usage of 100.00% with user/sys ratio of 0.477.

** Step 7. Test set w vectors generation takes 0.513s at CPU usage of 99.12% with user/sys ratio of 3.884.

** Step 8. OVA classification of the test set takes 1.922s at CPU usage of 98.22% with user/sys ratio of 117.000.

** Step 9. Measuring the performances of the classifiers takes 0.013s at CPU usage of 92.57% with user/sys ratio of infinity.

** Total running time is 50.423s for the training phase (Step 1 to Step 5 in which Step 4 is run without any ES which takes 39.486s). This suggests that:
1. Choosing the right representation of data like sparse vector representation really matters.
2. C output operations does not incur visible performance penalty as long as it is used infrequently.
3. C input operations are fast.
4. Using C++ high-level data structures does not incur visible performance penalty and they help a lot.

2. On ohsumed-all (http://disi.unitn.it/moschitti/corpora/ohsumed-all-docs.tar.gz)

* My implementation
** Step 1. Tokenization and TF calculation
*** With parallelism,
( "$exec_dir"/tok_tf $cat/$file > "$result_dir"/$directory/$file.tf ) &
results in:
real	1m53.594s
user	1m0.616s
sys	1m54.747s
